{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport math, os, random, cv2\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom IPython.display import HTML\n# Number of workers for dataloader\nworkers = 2\n\n# Batch size during training\nbatch_size = 64\n\n# Spatial size of training images. All images will be resized to this\n#   size using a transformer.\nimage_size = 64\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Number of training epochs\nnum_epochs = 100\n\n# Learning rate for optimizers\nlr = 0.0006\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1\n\n#id of the model to save\nidmodel=14","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-14T13:16:53.762863Z","iopub.execute_input":"2022-05-14T13:16:53.763238Z","iopub.status.idle":"2022-05-14T13:16:53.773449Z","shell.execute_reply.started":"2022-05-14T13:16:53.763204Z","shell.execute_reply":"2022-05-14T13:16:53.772411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip3 install torchgan","metadata":{"execution":{"iopub.status.busy":"2022-05-14T12:33:41.072454Z","iopub.execute_input":"2022-05-14T12:33:41.072805Z","iopub.status.idle":"2022-05-14T12:33:47.969622Z","shell.execute_reply.started":"2022-05-14T12:33:41.072771Z","shell.execute_reply":"2022-05-14T12:33:47.968349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchgan.layers import VirtualBatchNorm,MinibatchDiscrimination1d","metadata":{"execution":{"iopub.status.busy":"2022-05-14T12:33:47.974744Z","iopub.execute_input":"2022-05-14T12:33:47.977145Z","iopub.status.idle":"2022-05-14T12:33:47.983805Z","shell.execute_reply.started":"2022-05-14T12:33:47.977098Z","shell.execute_reply":"2022-05-14T12:33:47.982773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vizualisation","metadata":{}},{"cell_type":"code","source":"#from:https://www.kaggle.com/ihelon/monet-eda\ndef vis(path, n_images, is_random=True, figsize=(16, 16)):\n    plt.figure(figsize=figsize)\n    \n    w = int(n_images ** .5)\n    h = math.ceil(n_images / w)\n    \n    image_names = os.listdir(path)\n    \n    for i in range(n_images):\n        image_name = image_names[i]\n        if is_random:\n            image_name = random.choice(image_names)\n            \n        img = cv2.imread(os.path.join(path, image_name))\n        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        plt.subplot(h, w, i + 1)\n        plt.imshow(img)\n        plt.title(os.path.basename(os.path.normpath(path)))\n        plt.xticks([])\n        plt.yticks([])\n    \n    plt.show()\n    \nBASE_PATH = '../input/simpsons-faces/cropped'\nportrait_paint        = os.path.join(BASE_PATH,\"\")\n\nvis(portrait_paint, 12, is_random=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:13:56.748825Z","iopub.execute_input":"2022-05-14T13:13:56.749253Z","iopub.status.idle":"2022-05-14T13:14:18.422936Z","shell.execute_reply.started":"2022-05-14T13:13:56.749216Z","shell.execute_reply":"2022-05-14T13:14:18.408888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datalaoder","metadata":{}},{"cell_type":"code","source":"from distutils.dir_util import copy_tree\ncopy_tree('../input/simpsons-faces/cropped',\"simpsons/images\")\n#opy_tree('../input/wikiart-gangogh-creating-art-gan/portrait')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:14:18.425082Z","iopub.execute_input":"2022-05-14T13:14:18.425418Z","iopub.status.idle":"2022-05-14T13:15:29.245937Z","shell.execute_reply.started":"2022-05-14T13:14:18.425383Z","shell.execute_reply":"2022-05-14T13:15:29.245081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change root_path to \"../input/celebrities-100k/100k\" or \"portrait\" to use 100kceleb or portrait paintings\nroot_path=\"simpsons\"\n#root_path=\"../input/celebrities-100k/100k\"\ndataset = dset.ImageFolder(root=root_path,\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers,drop_last=True)\n\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:16:20.183048Z","iopub.execute_input":"2022-05-14T13:16:20.183372Z","iopub.status.idle":"2022-05-14T13:16:20.245018Z","shell.execute_reply.started":"2022-05-14T13:16:20.183342Z","shell.execute_reply":"2022-05-14T13:16:20.244269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:16:23.639198Z","iopub.execute_input":"2022-05-14T13:16:23.639525Z","iopub.status.idle":"2022-05-14T13:16:23.645141Z","shell.execute_reply.started":"2022-05-14T13:16:23.639495Z","shell.execute_reply":"2022-05-14T13:16:23.64418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:16:24.001301Z","iopub.execute_input":"2022-05-14T13:16:24.001613Z","iopub.status.idle":"2022-05-14T13:16:24.740755Z","shell.execute_reply.started":"2022-05-14T13:16:24.001582Z","shell.execute_reply":"2022-05-14T13:16:24.739856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Weight initialization\n\n\nUsing a custom weight initialization with a $\\mathcal{N}(0,0.02)$ distribution of weights weight (as suggested in https://arxiv.org/pdf/1511.06434.pdf - 4. DETAILS OF ADVERSARIAL TRAINING) for convolutionnal layers, a  $\\mathcal{N}(1,0.02)$ distribution of weights for BatchNorm Layers and 0 BatchNorm bias.\n\n* **Remark** :\nThe default initialization of BatchNorm layers is constant to 1 for weights and 0 for bias, but it seems that back then when the tutorial was made, the weights in BatchNorm Layers were initialized with a $\\mathcal{U}([0,1])$ distribution. The default initialization of Convolutionnal layers seem to be a modified version of the one suggested by Kaiming He (https://arxiv.org/pdf/1502.01852.pdf)","metadata":{}},{"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:16:28.262675Z","iopub.execute_input":"2022-05-14T13:16:28.263029Z","iopub.status.idle":"2022-05-14T13:16:28.271282Z","shell.execute_reply.started":"2022-05-14T13:16:28.262997Z","shell.execute_reply":"2022-05-14T13:16:28.270371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generator","metadata":{}},{"cell_type":"markdown","source":"The generator is made of 5 blocks of `ConvTranspose2d`, with `BatchNorm2d` and `ReLu` layers (except for the last one where the activation layer is `Tanh`)\n\nThe input data is a random noise (seen later) of size $1\\times 1$ with `nz` channels. \nWith the size ($4\\times 4$) of the kernel and the implicit (or controled) padding in the first `ConvTranspose2d` layer, then with the stride (an reduced padding) the image is upsampled to the $64\\times 64$ or `ngf` size with 3 channels.","metadata":{}},{"cell_type":"markdown","source":"For each `nn.ConvTranspose2d` the arguments are respectively nn.ConvTranspose2d(`in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `bias`)\n\nThe actual padding size is `dilation` $\\times ($ `kernel_size` $-1) - $ `padding` with default `dilation=1`. So the implicit default padding size is `kernel_size-1`,thus in our example there is an implicit padding for ` nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False) ` of $(3\\times 3)$\n\nFor this example, considering we have square images and `int` parameters only (so that they have the same size or effect on width and height), an input of size of size ($X \\times X\\times$`in_channels`) will have an output of size :\n\n<center> $(O,O,\\mathrm{out\\_channels})$ with $O=(X-1)\\times \\mathrm{stride} - 2 \\times \\mathrm{padding} + \\mathrm{dilation} \\times (\\mathrm{kernel\\_size} - 1) +1$ </center>","metadata":{}},{"cell_type":"markdown","source":"That is how `nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False)` or `nn.ConvTranspose2d( 100, 1024, 4, 1, 0, bias=False)` leads (using 1024 kernels) a $(1\\times 1 \\times 100)$ tensor to a tensor of size :\n\n<center> $(4,4,1024)$ with $4=(1-1)\\times \\mathrm{1} - 2 \\times \\mathrm{0} + \\mathrm{1} \\times (\\mathrm{4} - 1) +1$ </center>\n","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:16:33.248916Z","iopub.execute_input":"2022-05-14T13:16:33.249293Z","iopub.status.idle":"2022-05-14T13:16:33.258221Z","shell.execute_reply.started":"2022-05-14T13:16:33.249264Z","shell.execute_reply":"2022-05-14T13:16:33.257086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the generator\nnetG = Generator(ngpu).to(device)\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nnetG.apply(weights_init)\n\n# Print the model\nprint(netG)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:16:34.683034Z","iopub.execute_input":"2022-05-14T13:16:34.683378Z","iopub.status.idle":"2022-05-14T13:16:34.721974Z","shell.execute_reply.started":"2022-05-14T13:16:34.683347Z","shell.execute_reply":"2022-05-14T13:16:34.721077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discriminator","metadata":{}},{"cell_type":"markdown","source":"The discriminator is mostly made symmetrically to the Generator with 5 blocks of 2D-Convolution with`LeakyReLu` (which allows negative outputs, with a $0.2$ slope), except for the final convolution, and `BatchNorm2d`, except for the first and the final convolution. \n\nIt takes images of size $3\\times 64 \\times 64$ as inputs that are convoluted to $512 \\times 4 \\times 4$ entries then the last convolution yield a $1 \\times 1 \\times 1$ output where a `Sigmoid` activation is applied.","metadata":{}},{"cell_type":"markdown","source":"The `Conv2d` Layers (except the final one) are symetric to the transposed convolutions of the Generator, they use : `stride=2` and `padding=1` (as opposed to the `padding` in transposed convolutions, here the size of the padding is explicit and a line or column of padding of $0$s, default value, is added to the top, the bottom, the left and the right of the inputs). The stride of size 2 yields a quick reduction of the size of the image.\n\nThe last `Conv2d` layer is a $4\\times 4$ kernel applied on a $512 \\times 4 \\times 4$ input, thus it is a simple weighted sum for each channel, then a sum over the channels, plus a bias added at the end.","metadata":{}},{"cell_type":"markdown","source":"**EDIT** : \nA `main2` block is added (and the activation is removed from the `main` block). The result of the last convolution is set to a $4096\\times 1$ tensor, and used as as feature tensor, when needed for feature matching. Then the feature tensor is fed into the `main2` block where it is used with a `linear` layer and a `sigmoid` Activation.\nThis feature matching method is mentionned in *Improved Techniques for Training GANs* (Salimans et al., 2016), https://arxiv.org/pdf/1606.03498.pdf (the implementation of the feature matching was made using the Tensorflow framework, so this code is inspired by https://github.com/eli5168/improved_gan_pytorch)\n\nAn other improvement suggested in Salimans et al. was implemented in version 25, Minibatch Discrimination. It was manually added to the `forward` function, but it slowed down the process (3 hours for 15 epochs, instead of 15 minutes without). ","metadata":{}},{"cell_type":"markdown","source":"**EDIT** : \nAdded MinibatchDiscrimination using `torchgan`'s `MinibatchDiscrimination1d` layer","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            #nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False)\n            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 16),\n            # state size. (ndf*16) x 2 x 2\n        )\n        self.main2 = nn.Sequential(\n            nn.Linear(4096+128,1),\n            nn.Sigmoid()\n        )\n        self.minibatchdis=nn.Sequential(\n            MinibatchDiscrimination1d(in_features =4096,out_features =128,intermediate_features =16),\n        )\n    def forward(self, input,matching = False):\n        output = self.main(input)\n        feature = output.view(-1,4096)\n        #output = self.main2(feature)\n        \n        output = output.view(-1,4096)        \n        output=self.minibatchdis(output)\n        output=self.main2(output)\n        \n        if matching == True:\n            return feature,output\n        else:\n            return output #batch_size x 1 x 1 x 1 => batch_size","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:16:41.208965Z","iopub.execute_input":"2022-05-14T13:16:41.209296Z","iopub.status.idle":"2022-05-14T13:16:41.220975Z","shell.execute_reply.started":"2022-05-14T13:16:41.209266Z","shell.execute_reply":"2022-05-14T13:16:41.219357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"netD = Discriminator(ngpu).to(device)\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nnetD.apply(weights_init)\n\n# Print the model\nprint(netD)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:16:42.467498Z","iopub.execute_input":"2022-05-14T13:16:42.468044Z","iopub.status.idle":"2022-05-14T13:16:42.651092Z","shell.execute_reply.started":"2022-05-14T13:16:42.468006Z","shell.execute_reply":"2022-05-14T13:16:42.650177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"We are using the Binary Cross Entroy Loss function with its default parameters.\n\nThe `fixed_noise` is only used at the end of epochs or at each 500 iterationns for comparison purposes.\n\nThe optimizers used are `Adam` for both networks, there is nos scheduler for the learning rate (there are few epochs) and the fixed learning rate is lower than the suggested learning rate, the $\\beta_1$ parameter is at $0.5$ instead of $0.9$ which created to much instability according to the authors of https://arxiv.org/pdf/1511.06434.pdf. The $\\beta_2$ parameter is set at its default value.","metadata":{}},{"cell_type":"code","source":"# Initialize BCELoss function\ncriterion = nn.BCELoss()\ncriterionG = nn.MSELoss()\n\n# Create batch of latent vectors that we will use to visualize\n#  the progression of the generator\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# Establish convention for real and fake labels during training\n#Using one-sided-label smoothing, we choose 0.85 as real_label\nreal_label = 0.85\nfake_label = 0.\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\nschedulerD = optim.lr_scheduler.StepLR(optimizerD, step_size=int(num_epochs/4), gamma=0.5)\nschedulerG = optim.lr_scheduler.StepLR(optimizerG, step_size=int(num_epochs/4), gamma=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:16:45.570679Z","iopub.execute_input":"2022-05-14T13:16:45.571024Z","iopub.status.idle":"2022-05-14T13:16:45.57951Z","shell.execute_reply.started":"2022-05-14T13:16:45.570994Z","shell.execute_reply":"2022-05-14T13:16:45.578413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are using the Binary Cross Entroy Loss function with its default parameters on the discriminator outputs 3 separate times (for each input of each batch it computes $l=-[y.\\mathrm{log}(O) + (1-y).\\mathrm{log}(1-O)]$ with $O$ the ouput of the considered network, and then the mean across the batch) :\n1. On the discriminator for `errD_real` with the output $O=D(x)$ of the discriminator for $x$ real image and with the label $y=1$ thus $l=-\\mathrm{log}(D(x))$\n2. On the discriminator for `errD_fake` with the output $O=D(G(z))$ with $G$ the generator, and $z$ a random noise used as input for the generator, and with the label $y=0$ thus $l=-\\mathrm{log}(1-D(G(z)))$\n3. On the discriminator for `errG` with the output $O=D(G(z))$ and with the label $y=1$ thus $l=-\\mathrm{log}(D(G(z)))$","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Loop\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\nD_G_z1s=[]\nD_G_z2s=[]\nD_xs=[]\niters = 0\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        \n        ## Train with all-real batch\n        netD.zero_grad()\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        \n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        errD_real = criterion(output, label)\n        \n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        \n        #Calculate the mean value of D(x) with x real images\n        D_x = output.mean().item()\n\n              \n        ## Train with all-fake batch\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        fake = netG(noise) #generating the fake images\n        label.fill_(fake_label) #replace the content of the previous label variable\n\n        # Forward pass fake batch through D\n        output = netD(fake.detach()).view(-1)\n        errD_fake = criterion(output, label)\n        \n        # Calculate the gradients for this batch\n        errD_fake.backward()\n        \n        #Calculate the mean value of D(G(z)) with G(z) generated image\n        D_G_z1 = output.mean().item()\n        \n        # Add the gradients from the all-real and all-fake batches\n        errD = errD_real + errD_fake\n        \n        # Update D\n        optimizerD.step()\n        \n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad() #reset G gradient\n        label.fill_(real_label)  # fake labels are real for generator cost\n        \n        ##Feature matching\n        feature_real,_=netD(real_cpu,matching=True)\n        feature_fake,output=netD(fake,matching=True)\n        feature_real=torch.mean(feature_real,0)\n        feature_fake=torch.mean(feature_fake,0)\n        \n        errG=criterionG(feature_fake,feature_real.detach())\n        \n        # Perform another forward pass of all-fake batch through D\n        #output = netD(fake).view(-1)\n        \n        # Calculate G's loss based on this output\n        errG += criterion(output.view(-1), label)\n        \n        # Calculate gradients for G\n        errG.backward()\n        \n        #Calculate the mean value of D(G(z)) with G(z) generated image\n        output = netD(fake).view(-1)        \n        D_G_z2 = output.mean().item()\n\n        # Update G\n        optimizerG.step()\n\n        # Output training stats\n        if i % 1 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        D_G_z1s.append(D_G_z1)\n        D_G_z2s.append(D_G_z2)\n        D_xs.append(D_x)      \n        \n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1\n        \n    schedulerG.step()\n    schedulerD.step()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:17:05.006819Z","iopub.execute_input":"2022-05-14T13:17:05.007154Z","iopub.status.idle":"2022-05-14T13:54:22.145586Z","shell.execute_reply.started":"2022-05-14T13:17:05.007123Z","shell.execute_reply":"2022-05-14T13:54:22.144591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the model","metadata":{}},{"cell_type":"code","source":"#os.makedirs(\"models/netD\")\n#os.makedirs(\"models/netGS\")\n#os.makedirs(\"models/netDS\")\n\n#torch.save(netD.state_dict(), \"models/netDS/netD_\"+str(idmodel)+\".pt\")\n#torch.save(netG.state_dict(), \"models/netGS/netG_\"+str(idmodel)+\".pt\")","metadata":{"execution":{"iopub.status.busy":"2022-05-14T14:16:58.458358Z","iopub.execute_input":"2022-05-14T14:16:58.458717Z","iopub.status.idle":"2022-05-14T14:16:58.798333Z","shell.execute_reply.started":"2022-05-14T14:16:58.458678Z","shell.execute_reply":"2022-05-14T14:16:58.797555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T14:12:45.050375Z","iopub.execute_input":"2022-05-14T14:12:45.050897Z","iopub.status.idle":"2022-05-14T14:12:45.303981Z","shell.execute_reply.started":"2022-05-14T14:12:45.05085Z","shell.execute_reply":"2022-05-14T14:12:45.303031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title(\"Disciminator mean values\")\nplt.plot(D_G_z1s,label=\"D(G(z)) mean values (1)\")\nplt.plot(D_G_z2s,label=\"D(G(z)) mean values (2)\")\nplt.plot(D_xs,label=\"D(x) mean values\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T14:12:47.398965Z","iopub.execute_input":"2022-05-14T14:12:47.399292Z","iopub.status.idle":"2022-05-14T14:12:48.037677Z","shell.execute_reply.started":"2022-05-14T14:12:47.399254Z","shell.execute_reply":"2022-05-14T14:12:48.036771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%capture\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","metadata":{"execution":{"iopub.status.busy":"2022-05-14T14:12:52.417459Z","iopub.execute_input":"2022-05-14T14:12:52.417985Z","iopub.status.idle":"2022-05-14T14:12:59.725432Z","shell.execute_reply.started":"2022-05-14T14:12:52.41794Z","shell.execute_reply":"2022-05-14T14:12:59.724272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grab a batch of real images from the dataloader\nreal_batch = next(iter(dataloader))\n\n# Plot the real images\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# Plot the fake images from the last epoch\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T14:13:40.014522Z","iopub.execute_input":"2022-05-14T14:13:40.014875Z","iopub.status.idle":"2022-05-14T14:13:40.90455Z","shell.execute_reply.started":"2022-05-14T14:13:40.014839Z","shell.execute_reply":"2022-05-14T14:13:40.903652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(np.transpose(img_list[-1][:,66:132,66:132],(1,2,0)))","metadata":{"execution":{"iopub.status.busy":"2022-05-14T14:18:22.446419Z","iopub.execute_input":"2022-05-14T14:18:22.446971Z","iopub.status.idle":"2022-05-14T14:18:22.579871Z","shell.execute_reply.started":"2022-05-14T14:18:22.446931Z","shell.execute_reply":"2022-05-14T14:18:22.579084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove images from the ouput folder","metadata":{}},{"cell_type":"markdown","source":"mydir = \"./simpson/images\"\n\nfilelist = [ f for f in os.listdir(mydir)]\nfor f in filelist:\n    os.remove(os.path.join(mydir, f))","metadata":{"execution":{"iopub.status.busy":"2022-05-14T13:10:04.060939Z","iopub.status.idle":"2022-05-14T13:10:04.061689Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}